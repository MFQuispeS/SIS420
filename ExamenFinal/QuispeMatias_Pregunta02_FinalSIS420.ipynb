{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4MTHlZO9KOS",
        "outputId": "324cdc09-5252-4a02-acc1-e9c7b0f6fbd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0  Month        Age  Occupation  Annual_Income  \\\n",
            "0           1      2  23.000000          13       19114.12   \n",
            "1           2      6  34.429817          13       19114.12   \n",
            "2           3      0  23.000000          13       19114.12   \n",
            "3           4      7  23.000000          13       19114.12   \n",
            "4           5      5  23.000000          13       19114.12   \n",
            "\n",
            "   Monthly_Inhand_Salary  Num_Bank_Accounts  Num_Credit_Card  Interest_Rate  \\\n",
            "0            4194.170850                3.0              4.0            3.0   \n",
            "1            4194.170850                3.0              4.0            3.0   \n",
            "2            4194.170850                3.0              4.0            3.0   \n",
            "3            1824.843333                3.0              4.0            3.0   \n",
            "4            4194.170850                3.0              4.0            3.0   \n",
            "\n",
            "   Num_of_Loan  ...  Credit_Mix  Outstanding_Debt  Credit_Utilization_Ratio  \\\n",
            "0          4.0  ...           1            809.98                 31.944960   \n",
            "1          4.0  ...           1            809.98                 28.609352   \n",
            "2          4.0  ...           1            809.98                 31.377862   \n",
            "3          4.0  ...           1            809.98                 24.797347   \n",
            "4          4.0  ...           1            809.98                 27.262259   \n",
            "\n",
            "   Credit_History_Age  Payment_of_Min_Amount  Total_EMI_per_month  \\\n",
            "0                   0                      1            49.574949   \n",
            "1                 267                      1            49.574949   \n",
            "2                 268                      1            49.574949   \n",
            "3                 269                      1            49.574949   \n",
            "4                 270                      1            49.574949   \n",
            "\n",
            "   Amount_invested_monthly  Payment_Behaviour  Monthly_Balance  Credit_Score  \n",
            "0               118.280222                  3       284.629162             2  \n",
            "1                81.699521                  4       331.209863             2  \n",
            "2               199.458074                  5       223.451310             2  \n",
            "3                41.420153                  1       341.489231             2  \n",
            "4                62.430172                  6       340.479212             2  \n",
            "\n",
            "[5 rows x 25 columns]\n"
          ]
        }
      ],
      "source": [
        "# QUISPE SOLIZ MATIAS FERNANDO ----- INGENIERIA DE SISTEMAS ----- EXAMEN FINAL SIS420\n",
        "\n",
        "# Pregunta 2: Implementar una red neuronal, que permita realizar una regresión o clasificación de un problema que no podría ser resuelto por un simple perceptrón.\n",
        "\n",
        "# ---- Problema: Dataset multiclase , para clasificar el puntaje de crédito de los clientes de una compañia financiera ----\n",
        "\n",
        "# Cálculo científico y vectorial para python\n",
        "import numpy as np\n",
        "# Para manejo y analisis de estructuras de datos (En este caso para leer nuestro dataset de una manera mas comoda)\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Leemos nuestro dataset credit_score.csv (Dataset multiclase para predecir el puntaje o calificación de crédito\n",
        "# de un cliente en una compañia financiera, podria ser un banco por ejemplo)\n",
        "data = pd.read_csv('credit_score.csv', sep=',')\n",
        "# Imprimimos para observar los datos que contiene nuestro dataset\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Como podemos observar el dataset cuenta con una columna sin nombre, lo que nos dara problemas a la hora de trabajar con el dataset\n",
        "# por lo que optamos por quitar dicha columna, ya que tampoco es de relevancia en dicho dataset porque solo es un ID\n",
        "# ---Eliminamos la columna---\n",
        "data = data.drop('Unnamed: 0', axis=1)\n",
        "print(data.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XgY_bXLRNNl",
        "outputId": "0ebcb4dd-7529-4e27-f6c2-c264b7d7a8c1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Month        Age  Occupation  Annual_Income  Monthly_Inhand_Salary  \\\n",
            "0      2  23.000000          13       19114.12            4194.170850   \n",
            "1      6  34.429817          13       19114.12            4194.170850   \n",
            "2      0  23.000000          13       19114.12            4194.170850   \n",
            "3      7  23.000000          13       19114.12            1824.843333   \n",
            "4      5  23.000000          13       19114.12            4194.170850   \n",
            "\n",
            "   Num_Bank_Accounts  Num_Credit_Card  Interest_Rate  Num_of_Loan  \\\n",
            "0                3.0              4.0            3.0          4.0   \n",
            "1                3.0              4.0            3.0          4.0   \n",
            "2                3.0              4.0            3.0          4.0   \n",
            "3                3.0              4.0            3.0          4.0   \n",
            "4                3.0              4.0            3.0          4.0   \n",
            "\n",
            "   Type_of_Loan  ...  Credit_Mix  Outstanding_Debt  Credit_Utilization_Ratio  \\\n",
            "0           128  ...           1            809.98                 31.944960   \n",
            "1           128  ...           1            809.98                 28.609352   \n",
            "2           128  ...           1            809.98                 31.377862   \n",
            "3           128  ...           1            809.98                 24.797347   \n",
            "4           128  ...           1            809.98                 27.262259   \n",
            "\n",
            "   Credit_History_Age  Payment_of_Min_Amount  Total_EMI_per_month  \\\n",
            "0                   0                      1            49.574949   \n",
            "1                 267                      1            49.574949   \n",
            "2                 268                      1            49.574949   \n",
            "3                 269                      1            49.574949   \n",
            "4                 270                      1            49.574949   \n",
            "\n",
            "   Amount_invested_monthly  Payment_Behaviour  Monthly_Balance  Credit_Score  \n",
            "0               118.280222                  3       284.629162             2  \n",
            "1                81.699521                  4       331.209863             2  \n",
            "2               199.458074                  5       223.451310             2  \n",
            "3                41.420153                  1       341.489231             2  \n",
            "4                62.430172                  6       340.479212             2  \n",
            "\n",
            "[5 rows x 24 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora podemos observar que el dataset esta listo, para poder asignar las características a X y los valores a predecir a Y\n",
        "# Convertimos el DataFrame de Pandas en un arreglo de tipo NumPy para poder manejar comodamente\n",
        "data = data.to_numpy()\n",
        "# Asignamos los valores correspondientes a las variables X y Y, usando 70000 datos de nuestro dataset, tomando desde el inicio hasta 70000\n",
        "X, Y = (data[:70000, :23], data[:70000, 23])\n",
        "print(X)\n",
        "print(Y)\n",
        "\n",
        "\n",
        "# Normalizamos los datos haciendo uso de la media y la desviación estándar\n",
        "X_mean, X_std = X.mean(axis=0), X.std(axis=0)\n",
        "X_norm = (X - X_mean) / X_std\n",
        "# Verificamos que tengan las mismas dimensiones\n",
        "X.shape, Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyN_ByvBSFiy",
        "outputId": "533a331c-2c79-42ef-e6e0-34a84d18d4cc"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2.          23.          13.         ... 118.28022162   3.\n",
            "  284.6291625 ]\n",
            " [  6.          34.42981709  13.         ...  81.69952126   4.\n",
            "  331.20986285]\n",
            " [  0.          23.          13.         ... 199.45807439   5.\n",
            "  223.45130973]\n",
            " ...\n",
            " [  5.          49.           4.         ... 637.41299841   1.\n",
            "  285.71779825]\n",
            " [  4.          49.           4.         ...  30.94203438   5.\n",
            "  304.82771624]\n",
            " [  1.          50.           4.         ...  36.97395846   5.\n",
            "  298.79579216]]\n",
            "[2. 2. 2. ... 2. 1. 1.]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((70000, 23), (70000,))"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-10T13:13:12.785890Z",
          "start_time": "2020-08-10T13:13:12.775889Z"
        },
        "id": "84hPkqazMkir"
      },
      "outputs": [],
      "source": [
        "# Como ya tenemos listo nuestro dataset para ser utilizado procedemos con el siguiente paso\n",
        "\n",
        "# Haremos uso de una red neuronal para nuestro modelo\n",
        "\n",
        "# Primero definimos nuestra clase de Perceptrón Multicapa o MLP por sus siglas en ingles Multi Layer Perceptron\n",
        "# que nos servirá para resolver nuestro problema de clasificación multiclase de puntaje o calificación de credito de clientes de una compañia financiera\n",
        "\n",
        "# La clase estará formada por una lista de capas (layers)\n",
        "# También con la seccion de \"for layer y self.layers:\" nos encargaremos de la salida del modelo aplicando cada capa de manera secuencial recorriendo todas las capas\n",
        "class MLP: # Perceptrón Multicapa\n",
        "    def __init__(self, layers):\n",
        "        # el Perceptrón Multicapa es una lista de capas\n",
        "        self.layers = layers\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-10T13:13:13.130767Z",
          "start_time": "2020-08-10T13:13:13.123088Z"
        },
        "id": "X5jAi299Mkis"
      },
      "outputs": [],
      "source": [
        "# Con esta clase definimos las diferentes capas que usaremos para nuestro modelo\n",
        "# Para empezar tendremos una clase base donde estaran los parametro entrenables de la capa en cuestion, asi como tambien sus gradientes\n",
        "\n",
        "class Layer():\n",
        "    def __init__(self):\n",
        "        # Lista de parametros con todos los parametros de la capa\n",
        "        self.params = []\n",
        "        # Lista de gradientes que tendremos que aplicar a todos los parametros de la capa\n",
        "        self.grads = []\n",
        "\n",
        "    # por defecto, devolver los inputs pero cada capa hará algo diferente aquí\n",
        "    def __call__(self, x):\n",
        "        return x\n",
        "\n",
        "    # Recibe gradientes de las capas de arriba\n",
        "    # Con estos calcula sus propios gradientes y pasara los gradientes obtenidos a la capa de abajo\n",
        "    def backward(self, grad):\n",
        "        return grad\n",
        "\n",
        "    # Actualizara parametros si los hay, segun lo que diga el optimizador\n",
        "    def update(self, params):\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-10T13:13:13.413081Z",
          "start_time": "2020-08-10T13:13:13.396082Z"
        },
        "id": "lLLc0xzOMkis"
      },
      "outputs": [],
      "source": [
        "# Capa con parametros - Capa lineal\n",
        "# Esta clase representa la capa lineal (Perceptrón)\n",
        "# Lo usaremos para crear la secuencia de perceptrones (Perceptrón Multicapa)\n",
        "class Linear(Layer):\n",
        "    def __init__(self, d_in, d_out):\n",
        "        # Pesos de la capa (matriz que tendra dimensiones de entrada (d_in) y dimensiones de salida (d_out))\n",
        "        # Multiplicaremos estos pesos por los inputs\n",
        "        self.w = np.random.normal(loc=0.0,\n",
        "                                  scale=np.sqrt(2/(d_in+d_out)),\n",
        "                                  size=(d_in, d_out))\n",
        "        # Parámetro ballas\n",
        "        self.b = np.zeros(d_out)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.x = x\n",
        "        self.params = [self.w, self.b]\n",
        "        # Salida del preceptrón\n",
        "        # Producto de la matriz de inputs multiplicado por los pesos, mas el parametro ballas\n",
        "        return np.dot(x, self.w) + self.b\n",
        "\n",
        "    # Calculamos las derivadas de la funcion de costo con respecto a los parametros que vamos cambiando\n",
        "    def backward(self, grad_output):\n",
        "        # Gradientes para la capa siguiente (BACKPROP) obtenido del producto por los pesos por el gradiente que nos viene de la capa siguiente\n",
        "        grad = np.dot(grad_output, self.w.T)\n",
        "        # Gradientes de los pesos obtenido por los inputs por el gradiente de la capa siguiente\n",
        "        self.grad_w = np.dot(self.x.T, grad_output)\n",
        "        # Gradientes para actualizar pesos (Gradiente de ballas)\n",
        "        self.grad_b = grad_output.mean(axis=0)*self.x.shape[0]\n",
        "        self.grads = [self.grad_w, self.grad_b]\n",
        "        return grad\n",
        "\n",
        "    def update(self, params):\n",
        "        self.w, self.b = params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-10T13:13:13.553974Z",
          "start_time": "2020-08-10T13:13:13.534217Z"
        },
        "id": "z-n_93VrMkis"
      },
      "outputs": [],
      "source": [
        "# Esta capa va a ser una funcion de activacion\n",
        "\n",
        "class ReLU(Layer):\n",
        "    def __call__(self, x):\n",
        "        self.x = x\n",
        "        # Devuelve el valor maximo entre los inputs que recibe y 0\n",
        "        return np.maximum(0, x)\n",
        "    # Calculamos su derivada\n",
        "    def backward(self, grad_output):\n",
        "        # Derivada de la ReLU\n",
        "        grad = self.x > 0\n",
        "        # Usamos el gradiente de la capa siguiente y lo multiplicamos por la derivada de la ReLU\n",
        "        return grad_output*grad\n",
        "\n",
        "# Devuelve valores entre 0 y 1\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Los valores mas grandes los hace mas grandes en comparacion a los mas pequeños que los hace mas pequeños con relación al más grande\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.exp(x).sum(axis=-1,keepdims=True)\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __call__(self, x):\n",
        "        self.x = x\n",
        "        return sigmoid(x)\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        grad = sigmoid(self.x)*(1 - sigmoid(self.x))\n",
        "        return grad_output*grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-10T13:13:13.980603Z",
          "start_time": "2020-08-10T13:13:13.975081Z"
        },
        "id": "fL0FfbDRMkit"
      },
      "outputs": [],
      "source": [
        "# El optimizador que utilizaremos para nuestro modelo es el Stochastic Gradient Descent\n",
        "# Este optimizador calcula la derivada para cada elemento del dataset de manera independiente\n",
        "\n",
        "class SGD():\n",
        "    # En el inicializador le pasamos la red (net) y el learning rate (lr)\n",
        "    def __init__(self, net, lr):\n",
        "        self.net = net\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self):\n",
        "        # Iteramos por todas las capas de nuestra red neuronal\n",
        "        for layer in self.net.layers:\n",
        "            # En cada capa llamaremos a su propia funcion update\n",
        "            layer.update([\n",
        "                params - self.lr*grads\n",
        "                for params, grads in zip(layer.params, layer.grads)\n",
        "            ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-10T13:13:14.563035Z",
          "start_time": "2020-08-10T13:13:14.545038Z"
        },
        "id": "jnkv3upAMkiu"
      },
      "outputs": [],
      "source": [
        "# En esta clase definiremos nuestras funciones de perdida\n",
        "# Para nuestro caso como es un problema de clasificación multiclase\n",
        "# Haremos uso de de la funcion de perdida CrossEntropy\n",
        "\n",
        "\n",
        "# Clase base de funcion de perdida: Loss\n",
        "class Loss():\n",
        "    def __init__(self, net):\n",
        "        # Obtiene la red neuronal\n",
        "        self.net = net\n",
        "    # Desencadena todo el calculo de las derivadas\n",
        "    def backward(self):\n",
        "        # derivada de la loss function con respecto a la salida del Perceptron Multicapa\n",
        "        grad = self.grad_loss()\n",
        "        # BACKPROPAGATION\n",
        "        # Iteramos desde la ultima capa hasta llegar a la primera capa, hasta llegar a las entradas\n",
        "        # Propagando los gradientes desde la ultima capa hasta la primera\n",
        "        for layer in reversed(self.net.layers):\n",
        "            grad = layer.backward(grad)\n",
        "\n",
        "class CrossEntropy(Loss):\n",
        "    def __call__(self, output, target):\n",
        "        self.output, self.target = output, target\n",
        "        logits = output[np.arange(len(output)), target.astype(int)]\n",
        "        loss = - logits + np.log(np.sum(np.exp(output), axis=-1))\n",
        "        loss = loss.mean()\n",
        "        # Calculamos y devolvemos una loss (perdida)\n",
        "        return loss\n",
        "\n",
        "    # Cuanto necesitemos la derivada calcularemos gradiente de la funcion de perdida\n",
        "    def grad_loss(self):\n",
        "        answers = np.zeros_like(self.output)\n",
        "        answers[np.arange(len(self.output)), self.target.astype(int)] = 1\n",
        "        return (- answers + softmax(self.output)) / self.output.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-10T13:16:59.934417Z",
          "start_time": "2020-08-10T13:16:59.511295Z"
        },
        "id": "U4GacddOMki6",
        "outputId": "ded2ee34-f4eb-4b02-99be-2c1d78fcf23a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/300, Loss: 0.5501\n",
            "Epoch 100/300, Loss: 0.3938\n",
            "Epoch 150/300, Loss: 0.3098\n",
            "Epoch 200/300, Loss: 0.2617\n",
            "Epoch 250/300, Loss: 0.2320\n",
            "Epoch 300/300, Loss: 0.2128\n"
          ]
        }
      ],
      "source": [
        "# En esta parte especificamos los valores para que nuestro modelo entre de la manera adecuada\n",
        "# Podemos ajustar los valores de acuerdo a nuestro problema especifico, (clasificación multiclase de puntaje o calificación de credito de clientes de una compañia financiera)\n",
        "\n",
        "# Por ejemplo con D_in = X_norm.shape[1] estoy definiendo las dimensiones de entrada del modelo, en este caso son 23\n",
        "# Hacemos esto de manera dinamica, ya que obtiene el numero de columnas que tiene X_norm\n",
        "D_in = X_norm.shape[1]\n",
        "# Aqui especificamos la cantidad de neuronas que tendrán nuestras capas ocultas, podemos ir probando con diferentes valores\n",
        "H = 200\n",
        "# Aqi especificamos nuestra salida como 3 debido a que nuestro problema de clasificacion multiclase, contiene 3 clases diferentes\n",
        "D_out = 3\n",
        "\n",
        "# Aqui definimos nuestro Perceptrón Multicapa con las capas que querramos utilizar\n",
        "mlp = MLP([\n",
        "    # Capa oculta lineal\n",
        "    Linear(D_in, H),\n",
        "    # Funcion de activacion ReLU\n",
        "    ReLU(),\n",
        "    # Capa oculta lineal\n",
        "    Linear(H, H),\n",
        "    # Funcion de activacion ReLU\n",
        "    ReLU(),\n",
        "    # Capa oculta lineal\n",
        "    Linear(H, H),\n",
        "    # Funcion de activacion ReLU\n",
        "    ReLU(),\n",
        "    # Capa oculta lineal\n",
        "    Linear(H, H),\n",
        "    # Funcion de activacion ReLU\n",
        "    ReLU(),\n",
        "    # Capa de salida\n",
        "    Linear(H, D_out)\n",
        "])\n",
        "\n",
        "# Aqui definimos el optimizador, recordamos que estamos usando Stochastic Gradient Descent para nuestra optimizacion\n",
        "# le pasamos nuestra red neuronal definida como mlp (Perceptron Multicapa) y tambien la tasa de aprendizaje lr con un valor de 0.03 (learning rate)\n",
        "optimizer = SGD(mlp, lr=0.03)\n",
        "# Declaramos la funcion de perdida como CrossEntropy debido a que estamos frente a un problema de clasificación multiclase\n",
        "# de puntaje o calificación de credito de clientes de una compañia financiera, para ello le pasamos nuestra red neuronal\n",
        "loss = CrossEntropy(mlp)\n",
        "\n",
        "# Los epochs son el numero de interaciones que se llevaran a cabo\n",
        "# Podemos ir ajustando estos valores de acuerdo a nuestro problema\n",
        "epochs = 300\n",
        "# Especificamos antes el bath_size\n",
        "batch_size = 20\n",
        "\n",
        "# Aqui se especifica la cantidad del grupo de datos que se tomaran en cuenta\n",
        "batches = len(X) // batch_size\n",
        "# El log_each sirve para mostrarnos los avances de nuestro modelo cada tanto (valor de log_each) dentro de los epochs (iteraciones)\n",
        "log_each = 50\n",
        "l = []\n",
        "for e in range(1,epochs+1):\n",
        "    _l = []\n",
        "    for b in range(batches):\n",
        "        x = X_norm[b*batch_size:(b+1)*batch_size]\n",
        "        y = Y[b*batch_size:(b+1)*batch_size]\n",
        "        y_pred = mlp(x)\n",
        "        _l.append(loss(y_pred, y))\n",
        "        loss.backward()\n",
        "        optimizer.update()\n",
        "    l.append(np.mean(_l))\n",
        "    # Con esto podemos ver como es que va avanzando nuestro modelo mientras entrena\n",
        "    if not e % log_each:\n",
        "        print(f'Epoch {e}/{epochs}, Loss: {np.mean(l):.4f}')\n",
        "\n",
        "# Adjunto datos de prueba y resultados que obtuve para poder llegar a una perdida mas aceptable y un porcentaje de relacion obtenido en la siguiente celda despues de entrenar el modelo\n",
        "# Se probo cambiando varios valores uno a la vez para poder llegar al mejor conjunto de valores para dicho modelo de redes neuronales\n",
        "# Se logro deducir por ejemplo que:\n",
        "# Una tasa de aprendizaje muy baja no es conveniente, y tampoco es conveniente una mas alta\n",
        "# El valor mas ideal para este modelo es de lr=0.03\n",
        "# Se logro deducir tambien que mientrar mas neuronas hay en cada capa oculta mejor entrena el modelo\n",
        "# lo mismo con los epochs(epochs) mientras mas iteraciones se obtiene un mejor resultado pero puede llegar a tardar mucho cuando se exagera\n",
        "# el batch_size tambien llego a influenciar, ya que afecta un poco a la funcion de perdida y el procentaje de relacion, se llego a un valor ideal de batch_size=20\n",
        "\n",
        "# Tambien influyo bastante el hecho de agregar mas capas a nuestro modelo de Perceptron multicapa, gracias a ello se logro un mejor resultado.\n",
        "\n",
        "# Con 300 neuronas, 500 epochs y 50 de batch_size logre una perdida de 13% y un porcentaje de relacion de 100%\n",
        "# Con 100 neuronas, 100 epochs y 10 de batch_size logre una perdida de 56,07% y un porcentaje de relacion de 84.88%\n",
        "# Con 100 neuronas, 100 epochs y 10 de batch_size (lr=0.01) logre una perdida de 58,05% y un porcentaje de relacion de 86.56%\n",
        "# Con 100 neuronas, 100 epochs y 10 de batch_size (lr=0.03) logre una perdida de 55,91% y un porcentaje de relacion de 87.14%\n",
        "# Con 150 neuronas, 100 epochs y 10 de batch_size (lr=0.02) logre una perdida de 47,24% y un porcentaje de relacion de 93.46%\n",
        "# Con 150 neuronas, 100 epochs y 10 de batch_size (lr=0.03) logre una perdida de 47,21% y un porcentaje de relacion de 92.56%\n",
        "# Con 150 neuronas, 100 epochs y 10 de batch_size (lr=0.05) logre una perdida de 51,42% y un porcentaje de relacion de 88.50%\n",
        "# Con 150 neuronas, 100 epochs y 10 de batch_size (lr=0.04) logre una perdida de 50,20% y un porcentaje de relacion de 89.56%\n",
        "\n",
        "# Con 150 neuronas, 200 epochs y 10 de batch_size (lr=0.03) logre una perdida de 39,36% y un porcentaje de relacion de 93.00%\n",
        "# Con 150 neuronas, 200 epochs y 50 de batch_size (lr=0.03) logre una perdida de 42,77% y un porcentaje de relacion de 94.52%\n",
        "# Con 150 neuronas, 200 epochs y 20 de batch_size (lr=0.03) logre una perdida de 36,48% y un porcentaje de relacion de 96.08%\n",
        "# Con 150 neuronas, 200 epochs y 10 de batch_size (lr=0.03) logre una perdida de 39,78% y un porcentaje de relacion de 93.70%\n",
        "# Con 150 neuronas, 200 epochs y 30 de batch_size (lr=0.03) logre una perdida de 38.03% y un porcentaje de relacion de 95.96%\n",
        "# Con 150 neuronas, 200 epochs y 20 de batch_size (lr=0.03) logre una perdida de 34.10% y un porcentaje de relacion de 96.26% (Añadiendo una capa extra)\n",
        "# Con 150 neuronas, 300 epochs y 20 de batch_size (lr=0.03) logre una perdida de 30.50% y un porcentaje de relacion de 97.80% (Añadiendo una capa extra) (Tiempo: 33 min)\n",
        "# Con 200 neuronas, 300 epochs y 20 de batch_size (lr=0.03) logre una perdida de 21.50% y un porcentaje de relacion de 99.30% (Añadiendo una capa extra) (Tiempo: 42 min)\n",
        "# Con 200 neuronas, 350 epochs y 20 de batch_size (lr=0.03) logre una perdida de 20.44% y un porcentaje de relacion de 98.17% con 10k datos (Añadiendo una capa extra) (Tiempo: 49 min)\n",
        "# Con 200 neuronas, 300 epochs y 20 de batch_size (lr=0.03) logre una perdida de 18.93% y un porcentaje de relacion de 98.71% con 10k datos (Añadiendo una capa extra) (Tiempo: 32 min)\n",
        "# Con 200 neuronas, 300 epochs y 20 de batch_size (lr=0.03) logre una perdida de 21.28% y un porcentaje de relacion de 98.19% con 10k datos (Añadiendo una capa extra) (Tiempo: 24 min)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# En este bloque de codigo llevamos a cabo las pruebas de nuestro modelo ya entrenado\n",
        "# Para estas pruebas usamos los ultimos 10000 datos que tenemos dentro de nuestra variable X\n",
        "X_test = X[60000:, :].copy()\n",
        "\n",
        "# Se normalizan los datos de prueba si es necesario (utilizando la misma normalización que con los datos de entrenamiento) (media y desviacion estandar)\n",
        "X_test_norm = (X_test - X_mean) / X_std\n",
        "\n",
        "# Hacemos uso del modelo que entrenamos pasando los datos de prueba a través del modelo para obtener las predicciones\n",
        "y_pred_test = mlp(X_test_norm)\n",
        "\n",
        "# Convierte las predicciones a clases\n",
        "prediccion = np.argmax(y_pred_test, axis=1)\n",
        "\n",
        "# Imprimimos las predicciones hechas por nuestro modelo para verificar\n",
        "print(\"Predicciones del modelo:\")\n",
        "print(prediccion)\n",
        "print(\"Valores reales:\")\n",
        "print(Y[60000:])\n",
        "\n",
        "# Calculamos el porcentaje de relación que hay entre las predicciones y los valores reales\n",
        "total_ejemplos = len(prediccion)\n",
        "correlacion = np.sum(prediccion == Y[60000:])\n",
        "porcentaje = (correlacion / total_ejemplos) * 100\n",
        "\n",
        "# Mostramos el porcentaje de relacion que obtenemos con nuestro modelo entrenado\n",
        "print(\"Porcentaje de relación:\")\n",
        "print(f\"{porcentaje:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJuPM4D7O-0N",
        "outputId": "547bf4ff-0fff-439a-bd71-4951c48cb045"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicciones del modelo:\n",
            "[1 0 0 ... 2 1 1]\n",
            "Valores reales:\n",
            "[1. 0. 0. ... 2. 1. 1.]\n",
            "Porcentaje de relación:\n",
            "98.19%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "233.594px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}